\documentclass[10pt,twocolumn,letterpaper]{article}
 \pdfoutput=1
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{subcaption}
%\usepackage{slashbox}
\usepackage{color}
\usepackage[english]{babel}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{booktabs}
%\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabu,multirow}
\usepackage[toc,page]{appendix}
\DeclareCaptionLabelFormat{noname}{#2}
\captionsetup[algorithm]{labelformat=noname}

\newlength\myindent
\setlength\myindent{2em}
\newcommand\bindent[1][\myindent]{%
  \begingroup
  \setlength{\itemindent}{#1}
  \addtolength{\algorithmicindent}{#1}
}
\newcommand\eindent{\endgroup}


\usepackage{multirow}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
%\cvprfinalcopy

\def\cvprPaperID{3790} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}

\title{DeepAutoTrack (DAT): Vehicle Trajectory Prediction for Autonomous Driving}

\author{Di Wu, Zhennan Wang, Yi Tang,  Wenbin Zou, Xia Li, Chen Xu\\
Shenzhen University\thanks{Shenzhen Key Lab of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University.}\\
{\tt\small dwu,...@szu.edu.cn}}
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
% State the problem, your approach and solution, and the main contributions of the paper. Include little if any background and motivation. Be factual but comprehensive. The material in the abstract should not be repeated later word for word in the paper.

Vision-based deep neural networks are crucial components for autonomous driving in terms of visual understanding and decision making.
These models need to be temporally consistent and visually interpretable.
Most recent works focus on using static images for understanding visual semantics, ignoring the temporal consistency of driving conditions; or adopt end-to-end architectures for learning the pilot networks, providing limited interpretability.
In this paper, we raise the question of ``can we predict other traffic participant's future trajectory given previous egomotion visual input?".
The proposed system stems from the issue of human driver reaction time in the autonomous driving context.
Our dual-staged model firstly applies modern convolutional object detectors for spotting traffic participants (i.e., cars in this paper) and robustly tracks the targets of interest.
Then, a novel SEG-LSTM network is incorporated to fuse the multiple-streams from past frames and then predict targets' future trajectories.
We demonstrate the feasibility of predicting future trajectories of vehicles for assisting mediated perception.
We also show the effectiveness of combining various levels of abstractions(e.g., scene semantics, detection bounding boxes) further boost the prediction accuracy across diverse traffic conditions.
\end{abstract}
%%%%%%%%% BODY TEXT
%%% Andrje Karpathy's blog: http://karpathy.github.io/2016/09/07/phd/
%%% Another great resource on this topic is Tips for Writing Technical Papers from Jennifer Widom.
%%% https://cs.stanford.edu/people/widom/paper-writing.html
\section{Introduction}
\begin{figure}[t]
        \centering
        \includegraphics[width=0.42\textwidth]{figures/pull_figure.pdf}
        \caption{ \small{
        We propose a system to predict future trajectories of vehicles. Given historical information, obtained by detection, tracking and semantic segmentation, a multi-stream recurrent neural network is trained to generate future trajectory.}}
        \label{fig:pull_figure}
\end{figure}
%\textbf{\emph{1.1 What is the problem?}}

Currently, there are two major paradigms for autonomous driving systems built upon vision-based input~\cite{chen2015deepdriving}: mediated perception approaches that firstly explain the vision input and then parse the scene to make a driving policy (usually by a controller with if-then-else rules); and behaviour reflex approaches that directly map the vision input to a driving policy by a regressor.
In this paper, in the framework of first paradigm, we try to tackle the problem of predicting future trajectories of vehicle given its past information.

%\textbf{\emph{1.2. Why is it interesting and important?}}

 Being able to predict other traffic participants' future trajectories is important because it can help to prevent self-driving car from running into other cars. We need to know not just where other cars are, as in the localization case, but also how fast they are moving in order to avoid collisions.
For a human driver, reaction time is a crucial factor that includes recognizing the light has changed, deciding to continue or brake, and whether to stop engaging the brake (remove foot from accelerator and apply brake). Accident reconstruction specialists commonly use 1.5 seconds~\cite{mcgehee2000driver}.
Therefore, the ability to predict traffic participants' \emph{future} trajectories will greatly benefit the mediated perception approach's driving policy decision making.


%\textbf{\emph{1.3. Why is it hard? (Or why do naive approaches fail?)}}

Nonetheless, vehicles trajectory prediction is a challenging problem: by dissecting visual scene understanding individually (\emph{e.g.,} object detection, semantic segmentation, instance segmentation),  there are still many open challenges for computer vision community.
In addition, the scene parsing results contain quite some spurious information.
Existing methods for scene parsing also mainly focus on static images.
However, autonomous driving is intrinsically a dynamic problem. Human drivers make decisions by a sequence of input frames instead of one static frame.
Therefore, temporal consistency among continuous frames should also be taken into consideration when designing the interpretable system.



%\textbf{\emph{1.4. Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)}}
Vehicle trajectory prediction is also very related to visual tracking.
In traditional vision based tracking problems (\emph{e.g.},~\cite{wu2015object, mueller2016benchmark}) where target objects go through rather sporadic movements (usually the object trajectory is generated by artificial movement in order to test the robustness of the tracker), the prediction of the target's future trajectory is simply a moot problem.
However, traffic participants generally exhibit regular trajectories by obeying a large number of common sense rulesp (\emph{e.g.,} cars driving on the road, pedestrians walking on the sidewalks). Given law-abiding traffic participants, human drivers subconsciously project visual target's future trajectory.
Nonetheless, due to the missing bridge between the traditional visual tracking community and the current autonomous driving research community, there is a lack of proper metrics for evaluating the temporal prediction result.


%\textbf{\emph{1.5. What are the key components of my approach and results? Also include any specific limitations.}}

% Then have a final paragraph or subsection: "Summary of Contributions". It should list the major contributions in bullet form, mentioning in which sections they can be found. This material doubles as an outline of the rest of the paper, saving space and eliminating redundancy.
We address this issue through building a system that takes advantage of multiple streams with various levels of abstraction.
In our approach, there are three key components: traffic participants detection, instance tracking and future trajectory prediction as shown in Fig.~\ref{fig:pull_figure}.Our main contributions are as follows:
\begin{itemize}
\itemsep0em
\item We construct a time-series dataset for vehicle trajectory prediction based on the SYNTHIA dataset~\cite{ros2016synthia}\footnote{Dataset will be released upon paper publication.} and verify the feasibility of predicting vehicles' future trajectories for assisting mediated perception.

\item We take advantage of various levels of abstractions from multiple streams via recurrent network based temporal models and demonstrate the effectiveness of using ``auxiliary information"(\emph{e.g.}, scene semantics) further boost the prediction accuracy.

\item Preliminary results by incorporating ``social" factor demonstrate the potential of learning common sense rules with neighboring traffic participants in complex real world  environment.

\end{itemize}
%\begin{itemize}
%
%\item We verify the feasibility of predicting vehicles' future trajectories for assisting mediated perception. The proposed system stems from the issue of human driver reaction time in the autonomous driving context.
%
%\item We present a novel ``tracking-by-detection" framework for robust vehicle tracking. By updating tracker's target representation via detection association, we also solve the problem of instance association between frames.
%
%\item We design various temporal models for the problem of predicting future trajectories based on historical data. Results show that the temporal model generating intermediate representation performs better than the frame-to-frame based temporal model.
%
%\item We demonstrate the effectiveness of using ``privileged information"(\emph{e.g.}, scene semantics) further boost the prediction accuracy.
%
%
%\item We construct a time-series dataset for vehicle trajectory prediction based on the SYNTHIA dataset~\cite{ros2016synthia} and  formalize the problem into a 3D occupancy grid problem given depth information.
%
%\end{itemize}

\section{Related Work}

%\textbf{\emph{1. Object detection, segmentation, instance segmentation}}
%\subsection{Traffic participants detection and tracking: object detection and instance association}
\noindent \textbf{Traffic participants detection} Traffic participants detection is one key component of an autonomous driving system. Vehicle detection has been well studied for static high-way mounted cameras: ~\cite{hadi2014vehicle} present a concise overview of image processing methods and analysis tools before the ``deep learning eras".
%Typical algorithms output bounding boxes on detected vehicles.
A lot of progress has been made in recent years on object detection due to the use of convolutional neural networks (CNNs).
Modern object detectors~\cite{girshick2014rich, Girshick2015Fast, ren2015faster_nips, He2015Spatial, he2017mask, redmon2016you, liu2016ssd} are now good enough to be deployed in a consumer products. Faster R-CNN~\cite{ren2015faster_nips} proposes a region proposal network (RPN) and integrates RPN and Fast R-CNN~\cite{Girshick2015Fast} to build an end-to-end architecture. SSD \cite{liu2016ssd} similarly uses a series of boxes and directly gives the probability for each object class.

Typical algorithms output bounding boxes on detected objects for a static image. And the deep neural nets can be transferred and fine-tuned to another set of classes for traffic participants  we actually care about in the context of autonomous driving.
% on the R-CNN~\cite{girshick2014rich},
%Fast R-CNN~\cite{Girshick2015Fast}, Faster R-CNN~\cite{ren2015faster_nips}, Mask R-CNN~\cite{he2017mask} and  SSD~\cite{liu2016ssd} -- are now good enough to be deployed in a consumer products.
% Before the resurgence of convolutional neural network (CNN), Deformable Part Model (DPM) \cite{felzenszwalb2008discriminatively} and Selective Search \cite{Uijlings2013Selective} are the trend of this field.
% However, after the proposal of
%R-CNN~\cite{girshick2014rich} combines Selective Search and deep learning features to detect objects.
%% At the beginning, R-CNN is a step-by-step and inefficient method. Then,
%SPPNet \cite{He2015Spatial} exploits a spatial pyramid pooling layer to extract multi-scale deep features. Later Fast R-CNN~\cite{Girshick2015Fast} introduces the multi-task learning to fine-tune all layers in their network. At last, Faster R-CNN \cite{ren2015faster_nips} proposes a region proposal network (RPN) and integrates RPN and Fast R-CNN to build an end-to-end architecture. Meanwhile, SSD \cite{liu2016ssd} uses a series of boxes and directly gives the probability for each object class. In addition, YOLO \cite{redmon2016you} proposes a fast object detection method, which divides the input image into grids, then predicts object positions and confidences for each category in each grid.

%In the field of semantic segmentation, FCN~\cite{long2015fully} firstly transfers the fully-connected layers into convolution layers, turning the segmentation task into a pixel-wise classification task. Deeplab~\cite{chen2016deeplab} proposes a dilated convolution layer to explicitly control the resolution at
%which feature responses are computed within Deep Convolutional Neural Networks. DRN~\cite{yu2017dilated} further proposes a semantic network by integrating residual structure and dilated convolution.

%With the modification of neural network, conditional random field (CRF) is employed into refining the output of network in \cite{chen2016deeplab}. After that, CRF is merged into neural network to become an end-to-end architecture \cite{liu2015semantic,zheng2015conditional,arnab2016higher}


\noindent \textbf{Instance association} Instance association between frames is a prerequisite to solve the proposed intrinsically temporal problems.  3D scene flow is estimated in~\cite{behl2017bounding} by exploiting recognition to overcome the challenging scenarios in the presence of large displacement or local ambiguities.
%Their method achieve state-of-the-art performance on the KITTI2015 scene flow benchmark.
However, a total runtime of 10 minutes per frame is computationally prohibitive for the real-time deployment.
To accomplish the task of instance association between frames, we resort to the well studied field of visual tracking.
Discriminative Correlation Filters (DCF)~\cite{henriques2015high} have demonstrated excellent performance for high-speed generic visual object tracking.
Built upon their seminal work, there has been a plethora of recent improvements~\cite{hong2015online, ma2015hierarchical,danelljan2016beyond,held2016learning, wu2017kernalised,danelljan2017eco} relying on convolutional neural network (CNN) pretrained on ImageNet as a feature extractor for visual tracking.

Most of the modern trackers focus on the problem of ``class-agnostic" generic object tracking. In order to adapt to temporal changes, a continuous learning strategy is applied, where the model is updated rigorously in every frame. Such update is excessive and sensitive to sudden changed caused by, \emph{e.g.}, scale variations, deformations, and out-of-plane rotations. This excessive update strategy cause both lower frame-rates and degradation of robustness due to over-fitting to the recent frames~\cite{danelljan2017eco}.
In the context of autonomous driving, however, we know in advance the classes of object of interests for tracking (\emph{e.g.}, cars, pedestrians, cyclists). Therefore, with this extra source of information, we propose to use the neural nets object detection result as a more robust template to update the tacker's model to overcome the problem of model drifting. Hence the tracker is served solely as instance associator for the traffic participant.

\begin{figure*}[t]
        \centering
        \includegraphics[width=0.90\textwidth]{figures/framework.pdf}
        \caption{
        \small{The framework for the proposed vehicle future trajectory prediction system. The input is the sequential data (RGB and/or depth maps) up to the current time step obtained from the host car. Target vehicles' sequential bounding boxes are extracted by detection \& tracking module. Along with scene semantics as an auxiliary information, multi-stream inputs are fed into an LSTM to predict future trajectories taking advantages of various levels of abstractions.}
        }
        \label{fig:framework}
\end{figure*}
%\noindent \textbf{RNN for sequence prediction}

%Recently, RNN(recurrent network) and its variant LSTM(long short-term memory) have been successfully used in autonomous driving, due to its advantages for modeling sequential data in vision problems.
\noindent \textbf{Activity forecasting} Activity forecasting is the task of predicting the motion or the action to be carried out by objects in a video.
A body of work~\cite{chalasani2013deep, clark2013whatever, egner2010expectation} explores various hierarchical models in face of different quality of data representation.
 Recently recurrent neural networks (RNN) and their variants including Long Short Term Memory (LSTM)~\cite{hochreiter1997long} and Gated Recurrent Units (GRU)~\cite{chung2014empirical} have proven to be very successful for sequence prediction tasks.
 An action-conditioned video prediction model~\cite{finn2016unsupervised} that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames is proposed to learn from unsupervised video data.
 %The discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence when using a recurrent neural network.p
 A curriculum learning strategy is proposed in~\cite{bengio2015scheduled} to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead.
A combination of a fully-convolutional network and an LSTM is proposed in~\cite{xu2017end} to learn from large-scale vehicle action data and  is trained as a pilot network in the context of behaviour reflex approaches. Their goal is to predict future egomotion including multi-modal discrete and continuous driving behaviors.
Inverse turning radius is predicted by a LSTM network in~\cite{kim2017interpretable}: a heat map of attention is generated at each time step conditioned on the previous hidden states as a more succinct visual explanations.

More recent works also attempt to take the ``social factor" into consideration. Social-LSTM~\cite{alahi2016social} focus on modeling dynamics crowd interactions for path prediction by introducing a ``social tensor" from a spatial-temporal neighborhood. Based on the social-LSTM, an attention mechanism is introduced in ~\cite{social_lstm_2018} built upon the high-level spatio-temporal graphs proposed in~\cite{jain2016structural}.
%~\cite{duself} used 3D convolutional layers to extract visual features, then fed them into LSTM layers to capture the sequential relation. In addition to using deep features, ~\cite{koutnik2013evolving} trained a large recurrent neural network using a reinforcement learning approach to map images directly to steering angles, with the purpose to keep the car on track.


\noindent \textbf{Datasets for autonomous driving} Vision-based semantic segmentation in urban scene is a key functionality for autonomous driving.
%Camvid dataset~\cite{Camvid} consists of a set of monocular images taken in Cambridge UK. However, only 701 images contain pixel-level annotations over a total of 32 categories (combining objects and architectural scenes).
%Similarly, Daimler Urban Segmentation dataset~\cite{scharwachter2013efficient} contains 500 fully labelled monochrome frames for 5 categories.
KITTI benchmark suite~\cite{Geiger2013IJRR} provides a large amount of images of urban scene from Karlsruhe, Germany, with ground truth data for odometry, object detection, tracking, scene flow and stereo benchmarks. However, a limited 430 labelled images are provided for semantic segmentation.
%A common limitation of the aforementioned datasets is the bias introduced by the acquisition of images in a specific city. The LabelMe~\cite{russell2008labelme} project offers the solution by offering around 1,000 fully annotated images of urban environments around the world and more than 3,000 images with partial(noisy) annotations.
More recently, larger projects are constructed:
Cityscapes dataset~\cite{Cordts2016Cityscapes} which consists of a collection of images acquired in 50 cities around Germany, Switzerland and France in difference seasons, and having 5,000 images with fine annotations and 20,000 with coarse annotations over a total of 30 classes. Comma.ai~\cite{santana2016learning} provides 7 and a quarter hours of largely highway driving. Udacity~\cite{udacity} dataset includes 65,000 labels across 9,423 frames at full resolution of $1920\times1200$ at 2Hz.
The use of synthetic data has increased considerably in recent years within computer vision community.
CARLA~\cite{CARLA} and Sim4CV~\cite{Sim4CV} are two recent endeavours that take advantage of simulated environment to support development, training and validation of autonomous urban driving systems.
A synthetic dataset named SYNTHIA~\cite{mueller2016benchmark, HernandezBMVC17} is collected for investigating how useful synthetic images can be for semantic segmentation.
%In~\cite{kaneva2011evaluation}, the authors used a photorealistic virtual world to evaluate the performance of image features under scene changes and image transformations. Synthetic data has also been used for skeleton joints estimation~\cite{shotton2013real}, allowing the classifier invariant to pose, body shape, clothing, etc.
Synthetic dataset has an obvious advantage: a fine annotated image in Cityscape dataset requires on average 1.5 hours which is very labor intensive. Thus, the cost of scaling large project would require a prohibitive economic investment in order to capture images from a larger variety of countries, in different seasons and different traffic conditions.
%For these reasons, a novel synthetic dataset of urban scene called SYNTHIA~\cite{ros2016synthia} is proposed to use synthetic imagery that simulate real urban scenes in a vast variety of conditions and produce the appropriate annotations.
%This dataset is a large collection of images with high variability due to changes in illumination, textures, pose of dynamic objects and camera view-points.

\section{Proposed Algorithm}

We first formalized the proposed problem and describe the overall framework as shown in Fig.~\ref{fig:framework}.

\subsection{Problem formulation}
\begin{figure}[t]
        \centering
        \includegraphics[width=0.45\textwidth]{figures/network_detail.pdf}
        \caption{
        \small{Illustrations of three temporal networks. $\mathcal{J_H}$ represents historical trajectories, $\mathcal{J_F}$ is the $T$ time step future trajectories to be predicted.  Left: Frame-to-Frame (FtF); Middle: Frame-to-Sequence (FtS); Right: SEG-LSTM, with auxiliary information as input $\mathcal{A}$. The social tensor $\mathcal{S}$ can be inserted into existing recurrent networks .}
        }
        \label{fig:lstm}
\end{figure}

We propose to learn a generic approach from history information and formulate the problem as predicting future traffic participants' trajectories.
%Our work is most related to~\cite{xu2017end} where their problem was to predict future (next 1/3rd second) feasible actions of egomotion from a motion reflex perspective.
%We, instead,  predict other traffic participants' future trajectories in a longer time span (next 1.6 second).
Formally, our problem can be defined as a mapping $\mathcal{M}$ between historical trajectory $\mathcal{J_H}=\{j_p^t\}, t=1,\dots, T $ with auxiliary information $ \mathcal{A}=\{a_p^t\},t=1,\dots, T $ for traffic participant $p$ and the future trajectory $\mathcal{J_F}=\{j_p^t\}, t={T+1}, \ldots, T_F$:

\begin{equation}
\mathcal{J_F} \leftarrow \bm{\mathcal{M}}_p(j, a): \mathcal{J_H} \times \mathcal{A}
\label{eq:mapping}
\end{equation}
where the traffic participants $p$ can be vehicles, cyclists, pedestrians, \emph{etc}. In this paper, we focus on vehicle as the sole traffic participant category and ignore the subscript $p$ from now on.
$T_F$ is the length of future trajectory to be predicted (corresponding to the total driver reaction time).
Historical and future trajectories are defined in a 3D occupancy grid:
\begin{equation}
\mathcal{J_H, J_F} \in \Re^{6} =  \{x, y, w, h, d_{min}, d_{max}\}
\label{eq:mapping2}
\end{equation}
where $\{x,y,w,h\}$ define target's 2D boundingbox in image pixel space and $\{d_{min}, d_{max}\}$ define the minimum and maximum relative distance between host car and the tracking target (acquired from the depth camera). Given RGB cameras' focal length $f$, the pixel space 2D boundingbox can be projected into 3D real-world coordinates $\{x_r,y_r,w_r,h_r\}$ as:
\begin{align}
    x_r = \frac{x}{f}d &, y_r = \frac{y}{f} d  \label{eq:focal_length_1}  \\
    w_r = \frac{w}{f}d &, h_r = \frac{h}{f} d
    \label{eq:focal_length_2}
\end{align}
where $d$ is distance between  the center of the camera lens and the center of target. We use the minimum relative distance $d_{min}$ as a close approximation.

%Auxiliary information $\mathcal{A}$ could be scene semantics to regularize the path of future trajectory or car pose information to decide whether it's an approaching car or a parallel driving car.

\begin{figure}[t]
        \centering
        \includegraphics[width=0.42\textwidth]{figures/SEGLSTM.pdf}
        \caption{
        \small{Diagram of SEG-LSTM model: semantic inputs are one-hot encoded images with channel number $\mathcal{C}$ as the number of semantic categories (we used only ``car" and ``road" as the two semantics used under the context of high-way driving). The higher level semantics are extracted through a four-layer CNN, producing a joint representation of the auxiliary semantic information and the co-ordinate trajectory information.}
        }
        \label{fig:SEGLSTM}
\end{figure}
\subsection{Sequence modeling}

For predicting the vehicle's future trajectory, we compare three temporal models based on LSTM cells. A ``social pooling tensor" that implicitly reasons about the motion of neighboring traffic participants can be inserted into existing recurrent network architectures without any extra training supervision or modification to the optimization process as shown in Fig.~\ref{fig:lstm}. The first two models take sequential 3D occupancy as input. In the third model, we  explore using semantic segmentation as a side ``auxiliary" information $\mathcal{A}$ to better guide the trajectory prediction.

\vspace{\baselineskip}
\noindent \textbf{Frame-to-Frame (FtF) model:}
is akin to  sequence generation model from Graves~\cite{graves2013generating}.  At test-time $t$, it samples current predicted 3D occupancy as feedback to the model for predicting next frame occupancy grid. The model is frame to frame Markovian and the next frame trajectory $j_{t+1}$ can be estimated as:
\begin{equation}
j_{t+1} \leftarrow  \bm{\mathcal{M}}(j_t, j_H): \mathcal{J_H}
\label{eq:FtF}
\end{equation}

\vspace{\baselineskip}
\noindent \textbf{Frame-to-Sequence (FtS) model:}
differs from the FtF model in that instead of sampling frame by frame for predicting future trajectories, FtS model directly predicts a trajectory sequence of length.
As it pointed out in~\cite{bengio2015scheduled} that FtF training paradigm, predicting one token at a time, conditioned on the state and the previous correct token, is different from how we actually use them during inference and thus is prone to the accumulation of errors along the decision paths.
Therefore, a curriculum learning approach is proposed to slowly change the training objective from an easy task, where the previous token is known, to a realistic one, where it is provided by the model itself.
The aforementioned learning strategy applies to inference for time-varying sequence where an \textless EOS\textgreater token is needed to signify the end of the sequence.
For our continuous prediction task, we stipulate the length of the prediction is predefined as $T_F$.
Hence we have the assumption that current state is complete, in the sense it contains all historical information required for predicting the future trajectory of the vehicle.
The whole trajectory  $j_{t+1, \ldots, T_F}$ to be predicted can be estimated as:
\begin{equation}
j_{t+1, \ldots, T_F}, \leftarrow  \bm{\mathcal{M}}(j_H): \mathcal{J_H}
\label{eq:FtS}
\end{equation}
The whole sequence inference paradigm is more stable than the first FtF approach because of the elimination of the dependency of previous token. Such dependency will introduce the discrepancy between how the model is used at training and inference. Inference with the whole sequence will generate smoother predictions.

\vspace{\baselineskip}
\noindent \textbf{SEG-LSTM model:}
 takes advantages of scene semantics from  historical frames as auxiliary information $\mathcal{A}$. The second LSTM layer fuses the output from the bottom-most LSTM that encodes co-ordinates embedding with higher level semantic information.
With auxiliary $\mathcal{A}$, the trajectory  $j_{t+1, \ldots, T_F}$  to be predicted can be estimated as:
\begin{equation}
j_{t+1, \ldots, T_F}, \leftarrow  \bm{\mathcal{M}}(j_H, a): \mathcal{J_H} \times convnet(\mathcal{A})
\label{eq:SEG-LSTM}
\end{equation}
 Semantic inputs are one-hot encoded images with channel number $\mathcal{C}$ as the number of semantic categories  as in Fig.~\ref{fig:SEGLSTM}. The higher level semantic is extracted through a four-layer CNN, producing a joint representation of the semantic auxiliary information and the co-ordinate trajectory information.
 As described above, the 3D occupancy input  has only six dimensions: $[x, y, w, h, d_{min}, d_{max}]$ encoding the co-ordinates information. So the bottom-most LSTM not only can learn the temporal information between time-varying trajectories,  but also can balance input dimensions of both co-ordinate information and the semantic information.
%Results show that the SEG-LSTM model performs better than the other two temporal model.

Note that this model differs from the way semantic information is used in~\cite{xu2017end}: instead of using semantic segmentation as a ``privilege information" which is missing at test time, SEG-LSTM directly takes historical scene semantics as input and directs the recurrent net future trajectory prediction.


\vspace{\baselineskip}
\noindent \textbf{Social tensor:} was firstly introduced in~\cite{alahi2016social} by observing the ``common sense" rules and social conventions from pedestrians in crowded public space. Naturally, in the context of autonomous driving, such observation is also complied: when there are multiple cars occupying on-going lanes, the social interactions between traffic participants confine the space for their possible trajectories. The social interactions can be encoded implicitly by using a social pooling layer: at every time-step, the LSTM cell receives pooled hidden-state information from the LSTM cells of neighbors. While pooling the information, the spatial information is preserved by grid based pooling. The hidden state $h^t_i$ of the LSTM at time $t$ captures the latent representation of the $i^{th}$ target in the scene. This representation can be shared with neighbors by building a social hidden tensor $s^i_t$:
\begin{equation}
s^i_t = \sum_{j \in \mathcal{N}_i} \mathbf{1}_{m,n} [x^j_t - x^i_t, y^j_t - y^i_t] \centerdot  h^j_{t-1}
\label{eq:social_tensor}
\end{equation}
where $\mathbf{1}_{m,n} [x^j_t - x^i_t, y^j_t - y^i_t]$ is an indicator function to check if $(x, y)$ is in the $(m, n)$ cell of the grid, and $\mathcal{N}_i$ is the set of neighbors corresponding to target $i$.
The trajectory  to be predicted can be estimated as:
\begin{equation}
j_{t+1, \ldots, T_F}, \leftarrow  \bm{\mathcal{M}}(j_H, s^i_H): \mathcal{J_H}  \times \mathcal{S}
\label{eq:FtS}
\end{equation}
This social pooling layer does not introduce any additional parameters and can be inserted in to the hidden layer of LSTM. An important distinction from the vanilla LSTM is that the hidden states of multiple LSTMs are coupled by the social pooling layer. At every time-step, the update are jointly back-propagated through multiple LSTMs in a scene as a neighboring batch update scheme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

We first provide the data collection procedure for vehicle trajectory prediction. And then we present the implementation details with performance analysis of individual modules. Quantitatively examination of SEG-LSTM architecture is also presented showing the advantages of using semantics directly as input can better assist the neural net's trajectory prediction.

\subsection{Dataset collection}


Currently, most public datasets with semantic labeling are single frame, static images. Continuous video streams are required for the purpose of car trajectory prediction.
We collect a car trajectory prediction dataset based on the SYNTHIA~\cite{ros2016synthia} dataset.
SYNTHIA dataset is a large corpus of synthetic images originally collected for the purpose of semantic segmentation of urban scenes generated by rendering a virtual city created with the Unity development platform.
The potential of this virtual world includes extension capabilities: new parts of the cities and road conditions can be easily generated by adding different setups. The major features of the collected dataset include: scene diversity (European style town, modern city, highway and green areas), variety of dynamic objects (cars, pedestrians and cyclists), multiple seasons (dedicated themes for winter, fall, spring and summer), lighting conditions and weather (dynamic lights and shadows, several day-time modes, rain mode and night mode).
There are more than 200,000 HD ($760\times1280$) photo-realistic frames from video streams.
Frames are acquired from multiple view-points (up to eight views per location), and each of the frames also contains an associated depth map. In our experiment, we used only left front camera view data.


\begin{algorithm}[t]
\begin{algorithmic}
\caption*{\textbf{Ground truth collection for car trajectory prediction}}\label{list:dataset_collection}
\small{
\STATE \textbf{Step 1} $\rightarrow $ acquiring traffic participants from ground truth annotations with only ``car" as instances.
\STATE \textbf{Step 2} {$\rightarrow $ acquiring frame-based instance information:
{
\bindent
  \IF{POR $> 0.2\%$}
    %\bindent[1.45cm]
    \STATE $\Rightarrow$ start tracking instance;
    \STATE $\Rightarrow$ collecting 3D tracking boundingbox: $[x, y, w, h, d_{min}, d_{max}]$;
    \STATE $\Rightarrow$ collecting semantic labeling image $I_{seg}$;
    \STATE $\Rightarrow$ collecting 2D car pose from RGB input $I_{car}$;
    %\eindent
  \ELSIF{POR $< 0.1\%$}
    \STATE $\Rightarrow$ stop tracking.
    \ENDIF\eindent}
  }
\STATE \textbf{Step 3} $\rightarrow $ splitting sequences into tracklets of 23 frames with stepsize 1:
{\bindent
    \STATE $\Rightarrow$  first 15 frames (5Hz, 3 sec) as training input;
  \STATE $\Rightarrow$  next 8 frames (1.6 sec) as the held-up future trajectory to be predicted.
  \eindent}
}
\end{algorithmic}
\end{algorithm}

The focus of this paper is on car trajectory prediction on highways which corresponds to sequence number 1 in the SYNTHIA dataset. We are especially concerned about the cars that are relatively close to the driver.
In order to decide quantitatively which vehicles are close to the host car, we define pixel occupancy ratio \emph{(POR)} as the ratio between the total number of pixels of the tracking vehicle and the total number of pixels of the camera input.
As in accordance with~\cite{chen2015deepdriving}, we set the reliable car perception as 30 meters so as to guarantee satisfactory control quality when the speed of the host car does not exceed 72km/h.
Since depth information is also provided alongside in the SYNTHIA dataset, we back-project the segmented instance region onto the depth maps and estimate that the POR of 0.2\% and 0.1\% correspond to roughly 30 meters and 100 meters of relative distance between the host car and the tracked vehicle.
Hence, we start tracking the target when the POR of the detected vehicle is larger than 0.2\%  so as to suffice the safety distance for driver to make timely reaction and stop tracking the vehicle when the POR is smaller than 0.1\% indicating the target car is too far to influence driving policy.
We follow the prediction time allocation as in~\cite{xu2017end} that the historical time-span is 3 seconds (corresponds to 15 frames with 5 Hz frame rate in the SYNTHIA dataset).
However, in contrast with~\cite{xu2017end} where only the next 1/3rd of a second is predicted, we strive to predict the next 1.6 second's future trajectory as mentioned previously that it's the reaction time accident reconstruction specialists commonly used~\cite{mcgehee2000driver}.

We follow the traditional paradigm for visual tracking to collect target's bounding box: $[x, y, w, h]$ of each frame. Moreover, the minimum relative distance $d_{min}$ and maximum relative distance $d_{max}$ can also be acquired from the depth camera. The train/valid/test are split as 80\%/10\%/10\% of the total tracklets and the total numbers are shown in Tab.~\ref{tab:dataset}.
%We list our logic for collecting continuous car tracking in List~\ref{list:dataset_collection}.
Some sample frames with semantic labels and depth information is shown in Fig.~\ref{fig:dataset}.


\begin{figure}[t]
        \centering
        \includegraphics[width=0.5\textwidth]{figures/dataset.pdf}
        \caption{
        \small{Examples of collected dataset for vehicle trajectory prediction. From top to bottom: RGB, semantic labeling, depth maps for three continuous frames under two dynamic light conditions.}
        }
        \label{fig:dataset}
\end{figure}

\begin{table}[t]\centering
\ra{1.}
\begin{tabu}{@{}llll@{}}\toprule
[-1pt] \tabucline[1pt]{1-4}
                & \#train & \#valid & \#test\\ \hline
 tracklets      &  10400    &  1300    & 1280 \\ \hline
 car detection  &   7832     & 976     & 986 \\ \hline
 semantic segmentation  &  7841  & 977  & 987 \\
 \hline
 \end{tabu}
%\end{tabular}
\caption{\small{Dataset statistics}}
\label{tab:dataset}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation Details}\label{sec:Implementations}

\subsubsection{Traffic participants detection}

To reliably detect traffic participants, we compared two state-of-the-art approaches: SSD~\cite{liu2016ssd} and Faster-RCNN~\cite{ren2015faster_nips}.
We use the pretrained network on the PASCAL VOC detection dataset~\cite{everingham2015pascal} which has 20 classes and fine-tune the network on the SYNTHIA with two classes: cars \emph{v.s.} background.
A comprehensive survey of trade-offs for modern convolutional object detectors is presented in~\cite{huang2017speed} and we refer keen readers to the aforementioned paper for a more complete comparison to achieve the right speed/memeory/accuracy balance for a given application and platform.

As is also pointed out in the paper~\cite{huang2017speed} that SSD, albeit is less advantageous in detecting small objects, it's very competitive for detecting larger objects. In this paper, we are mostly concerned about cars that are close to the driver. Hence, given 0.1\% POR as the cut-off threshold, the presented problem favors detectors that are robust in detecting larger objects.
Tab.~\ref{tab:ssd_fasterrcnn} verifies that SSD is indeed more competitive when objects of interests are large in our problem set.
We also compare the influence of the cut-off threshold for various confident scores and non-maximum suppression (NMS) thresholds. It can been seen that both meta-architectures are robust to cut-off confident scores.
Allowing larger NMS threshold enables the detector to have slightly higher recall, it comes at a cost of multiple redundant overlapping detections and much lower f-score overall. In the following experiment, we adopt SSD with a confident score of 0.5 as the cut-off threshold and 0.45 as the default jaccard overlap for NMS.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Instance association via tracking}

To associate traffic participants across different frames (\emph{i.e.}, instance association), we creatively combine detecting with tracking.
%The marriage between DCF~\cite{henriques2015high}, which has the advantage of being efficient in training translational images in the fourier space, and deep features, which excel at image representation, further advances the visual tracking community~\cite{qi2016hedged, danelljan2016eccv, wu2017kernalised, danelljan2017eco}. However, in the pursuit of ever increasing tracking accuracy, their characteristic speed and realtime capability have gradually faded.
%In the context of autonomous driving, accurate scale estimation of a target is also a prerequisite. Most state-of-the-art methods employ an exhaustive scale search to estimate the target size which is computationally expensive and struggles when encountering with large scale variations.
For this task, we adopt the real-time scale adaptive tracker fDSST~\cite{danelljan2017discriminative} that achieves 50 FPS with scale estimation.
For every newly detected target whose POR is larger than the predefined threshold of 0.2\%, we initiate the tracker with a unique target ID.

The tracker's target model is updated every time when there is a detection whose jaccard overlap is larger than 0.3.
We argue that force update of tracker model via the detection template is essential when the target undergo rapid out of plane rotations.  Trackers~\cite{qi2016hedged, danelljan2016eccv, wu2017kernalised, danelljan2017eco} focus on the ``class-agnostic" settings. In the context of autonomous driving, however, we know in advance the classes of object of interests for tracking. Therefore, with this extra source of information, we propose to use the neural nets object detection result as a more robust template to update the tacker’s model to overcome the problem of model drifting.
Hence the tracker is served solely as instance associator for the traffic participant.
In return, the instance association scheme helps to alleviate the problem of false positive detection (\emph{c.f.} Fig.~\ref{fig:Instance_association}) from the detection result.
The target's tracker will terminate under either of the following two conditions: if the target's POR is smaller than 0.1\% (\emph{i.e.}, the vehicle is too far way from the host car); or if there are more than 5 frames of detection absence  (\emph{i.e.}, the target could be one false positive detection from the vehicle detector).


\subsubsection{Trajectory prediction}

For the task of sequential prediction, both FtF and FtS share the same base network structure:
a 3-layer recurrent net with 100, 300, 300 LSTM cells.
For SEG-LSTM, since the focus of the scene context is high-way driving, auxiliary information is the scene semantics with number of channels $\mathcal{C}$ sets as 2 (``car" and ``road"). We adopt the state-of-the-art semantic segmentation networks: Dilated Residual Networks (DRN)~\cite{yu2017dilated} and retrain on the SYNTHIA corpus. DRN achieves 68\% and 82\% pixel mAP on the car and road categories. The input semantics $\mathcal{A}$ are one-hot encoded in the dimension of $95\times160\times2$.
The CNN for learning high level semantics is composed of 4 convolutional blocks. Each block includes a convolutional layer, a ReLU activation layer, a maxpooling layer and a dropout layer (as we have observed overfitting during network training). Each convolutional layer has four channels with kernel of size 3.

For the social tensor configuration, we incorporate all vehicles in the same time-step as their neighbors. In contrast with~\cite{alahi2016social} where a spatial constraint is required in order to be counted as a neighbor, all the vehicles in the same scene are relevant for the social interactions under our high-way driving context.
Grid size is set as 4 and the embedding size is 64 so the social tensor $\mathcal{S}$ is a $4\times 4\times 64$ matrix.
The learning is set to 0.001 and RMS-prop~\cite{tieleman2012lecture} with a gradient clipping of 3 to avoid divergence.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}[t]
%   \centering
%    \begin{subfigure}[c]{0.25\textwidth}
%    \centering
%    \includegraphics[width=7cm,height=5cm, clip]{figures/precision_plot_2d.png}
%    \caption{\small{Precision plot for 2D (pixel as x-axis)}}
%    \end{subfigure}%
%    \begin{subfigure}[c]{0.25\textwidth}
%    \centering
%        \includegraphics[width=7cm,height=5cm, clip]{figures/success_plot_2d.png}
%        \caption{\small{Success plot 2D ()}}
%    \end{subfigure}
%     \begin{subfigure}[c]{0.25\textwidth}
%     \includegraphics[width=7cm,height=5cm, clip]{figures/precision_plot_3d_20cm.png}
%    \caption{\small{Precision plot for 3D (centimeter as x-axis)}}
%    \end{subfigure}%
%    \begin{subfigure}[c]{0.25\textwidth}
%    \centering
%        \includegraphics[width=7cm,height=5cm, clip]{figures/success_plot_3d.png}
%        \caption{\small{Success plot 3D (IOU as x-axis)}}
%    \end{subfigure}
%\caption{\small{ 2D and 3D space precision plot and success plot.}}
%\label{fig:precision_plot_and_success_plot}
%\end{figure*}


%\setlength{\fboxsep}{1pt}%
%\setlength{\fboxrule}{1pt}%
\begin{figure*}[t]
       \centering
        \begin{subfigure}[c]{0.24\textwidth}
        \centering
                \includegraphics[width=4.2cm,height=3.5cm, clip]{figures/precision_plot_2d.png}
    \caption{\small{2D Precision plot}}
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \centering
                \includegraphics[width=4.2cm,height=3.5cm, clip]{figures/success_plot_2d.png}
        \caption{\small{2D success plot}}
        \end{subfigure}
       \begin{subfigure}[c]{0.24\textwidth}
        \centering
                \includegraphics[width=4.2cm,height=3.5cm, clip]{figures/precision_plot_3d_20cm.png}
    \caption{\small{3D precision plot}}
        \end{subfigure}
               \begin{subfigure}[c]{0.24\textwidth}
        \centering
                \includegraphics[width=4.2cm,height=3.5cm, clip]{figures/success_plot_3d.png}
        \caption{\small{3D Success plot}}
        \end{subfigure}

\caption{\small{ 2D and 3D space precision plot and success plot. x-axis for (a) to (d) are: pixel, IOU, centimeter and IOU.}}
\label{fig:precision_plot_and_success_plot}
\end{figure*}


\subsection{Evaluation}

\begin{figure*}[t]
        \centering
        \includegraphics[width=0.95\textwidth]{figures/prediction_2.pdf}
        \caption{
        \small{Visualization of prediction results. Top two rows show the advantage of using semantics in temporal model: {\color{red}Red: ground truth}; {\color{blue}Blue: SEG-LSTM}; {\color{green}Green: FtS}; {\color{yellow}Yellow: FtF}.
        SEG-LSTM can help detecting neighboring vehicles and restricts the viable path outside the space occupied by other vehicles (row 1) and confines the plausible car trajectory along the space above road (row 2).
        Row 3 and 4 show the benefit  of incorporating social factors.
        Row 3 is the visualization for vanilla LSTM. The bottom left car prediction (denoted by red dots) is considered in isolation and could surpass the host car, moving dangerously close to the two front cars. Row 4 is the visualization of social LSTM: by considering two front cars in the scene, the predicted keeps the red dot car in a safer distance from two front cars.
        }
        }
        \label{fig:final}
\end{figure*}


\begin{table}[t]
\small
   \centering
        %\begin{tabular}{|l *{2}{c}|}\hline
        %\begin{tabu}{|l *{2}{c}|}
        \begin{tabu}{@{}ccc@{}}\toprule
        [-1pt] \tabucline[1pt]{1-3}
            Configuration  &     mean coverage      & center error   \\
                       2D   &       (\%) &             (pixel)     \\ \hline
            {\small Kalman Filter}             &  54.31          &  24.84             \\
            {\small \textbf{FtF} }              &  66.47     &  23.43  \\
            {\small \textbf{FtS}  }              &  73.93      &   15.32  \\
            {\small \textbf{SEG-LSTM} }         &  \textbf{77.53  }    & \textbf{ 12.63 }   \\
          [-1pt] \tabucline[1pt]{1-3}
        \end{tabu}
        %\end{tabular}

    \caption{ {\small
    Results of 2D performance evaluation (pixel as unit) on mean coverage (higher is better) and center error (lower is better).
     }
          } \label{table_baseline_2d}
\end{table}
%%%%%%%%%%%%%%%%%


\begin{table}[t]
\small
   \centering
        %\begin{tabular}{|l *{2}{c}|}\hline
        %\begin{tabu}{|l *{2}{c}|}
        \begin{tabu}{@{}ccc@{}}\toprule
        [-1pt] \tabucline[1pt]{1-3}

           Configuration  &   mean coverage      & center error   \\
                       3D   &       (\%) &             (meter)     \\
                            \hline
            {\small \textbf{FtF} }              &    35.67    &  0.7122   \\
            {\small \textbf{FtS}  }              &   46.69	     &    0.3686 \\
            {\small \textbf{SEG-LSTM} }         &  \textbf{48.49}     &   \textbf{0.3298}   \\
          [-1pt] \tabucline[1pt]{1-3}
        \end{tabu}
        %\end{tabular}

    \caption{ {\small
    Results of 3D performance evaluation (centimeter as unit) on mean coverage (higher is better) and center error (lower is better).
     }} \label{table_baseline_3d}
\end{table}


%\vspace{\baselineskip}
\noindent \textbf{Baseline: Kalman filter}, also known as linear quadratic estimation, is a popular technique for estimating the state of a system~\cite{Thrun2016}. For the task of trajectory prediction, Kalman filters estimates a continuous state and gives a uni-modal distribution. The Kalman filter represents all distributions of the Gaussians and iterates two states: (1) measurement updates; (2) motion updates. In the baseline, we set
the initial uncertainly covariance as 1e4, measurement noise and motion noise are all set with unit value.
%We evaluation the trajectory both in 2D space (using pixel as unit distance) and 3D space (using meter as unit distance) as shown in Fig.~\ref{fig:evaluation}.

\noindent \textbf{2D Evaluation Methodology:}
Following the evaluation strategy  of~\cite{wu2013online}, all trajectories are compared using two measures: precision and success. Precision is measured as the distance between the centers of the ground truth bounding box and the corresponding tracker generated bounding box. The precision plot shows the percentage of tracker bounding boxed within a given threshold distance in pixels of the ground truth. To rank the prediction performance, the conventional threshold of 20 pixels (\emph{P20}) is adopted. Success is measured as the intersection over union of pixels. The success plot shows the percentage of tracker bounding boxes whose overlap score is larger than a given threshold and the trackers are ranked according to the Area Under Curve (\emph{AUC}) criteria.
% The 2D space evaluation result is shown in Fig.~\ref{fig:precision_plot_and_success_plot}(top) and Tab.~\ref{table_baseline_2d}.
%\vspace{\baselineskip}

\noindent \textbf{3D Evaluation Methodology:} Given depth map and the parameter of camera focal lens, we can formalize the problem into a 3D occupancy grid estimation. The real world coordinates $[x_r, y_r, w_r, h_r]$ can be obtained according to Eq.~\ref{eq:focal_length_1}~\ref{eq:focal_length_2}. The 3D jaccard index can be similarly formalized as the volume intersection over union: $\frac{V_g \cap V_p}{V_g \cup V_p}$, where $V_g, V_p$ are the ground truth and predicted space occupancy. 3D center error correspondingly is defined as the mean square root in 3D space.
%The 3D space evaluation result is shown in Fig.~\ref{fig:precision_plot_and_success_plot}(bottom) and Tab.~\ref{table_baseline_3d}.

%%%%%%%%%%%%%%%%%
\begin{table}[t]
\small
   \centering
        %\begin{tabular}{|l *{2}{c}|}\hline
        %\begin{tabu}{|l *{2}{c}|}
        \begin{tabu}{@{}ccc@{}}\toprule
        [-1pt] \tabucline[1pt]{1-3}
                                                    &   Ave. disp. error      & Final disp. error   \\ \hline
            {\small Vanilla LSTM }                 &     0.0514              &  0.0880   \\
            {\small \textbf{Social LSTM} }         &  \textbf{0.0494 }         & \textbf{0.0837}   \\
          [-1pt] \tabucline[1pt]{1-3}
        \end{tabu}
        %\end{tabular}

    \caption{ {\small Results comparing vanilla LSTM and social LSTM in  unit space. \emph{Ave. disp. error} is the mean square error over all estimated points of a trajectory and the true points. \emph{Final disp. error} is the distance beween the predicted final destination and the true final destination at the end of the prediction period $T_F$.
     }
    } \label{tab:social_lstm}
\end{table}

%\vspace{\baselineskip}
\noindent \textbf{Learning with auxiliary information:}
 The 2D and 3D evaluation results in Fig.~\ref{fig:precision_plot_and_success_plot} and Tab.~\ref{table_baseline_2d}  show that our proposed system consistently outperforms the baseline Kalman filter. Kalman filter is intrinsically a linear model and it's difficult to handle highly nonlinear driving trajectories in a relative coordinate.
 The auxiliary learning with semantic information in SEG-LSTM also consistently outperforms other two temporal models.
 Fig.~\ref{fig:final} shows the advantage of using SEG-LSTM: by utilizing the scene semantics (\emph{e.g.}, road, cars), it's able to direct the recurrent net to output the most plausible path for vehicles and avoid collision with neighboring vehicles.

%\vspace{\baselineskip}
\noindent \textbf{Learning with social tensor:}
On average, there is around 3 cars of interests in a high-way driving scene.
Tab.~\ref{tab:social_lstm} shows that considering social interactions between cars improves the prediction accuracy than the isolation case.
And we believe that in a more complex social scene such as urban driving, the improvement will be more pronounced.



\section{Conclusions}
In this paper, mimicking the human driver reaction time, we advocate to solve the problem of predicting traffic participants future trajectory given historical information.
We propose a two-staged system incorporating various visual inputs with a temporally recurrent neural network. The recurrent network takes semantic information as an auxiliary input and further improve the predicting accuracy with the incorporation of social factor.
Future works include incorporating vehicle pose from the RGB/depth input as extra source of ``auxiliary information" and conducting experiments on the real-world images.


%\section*{Details of the Code}
%\footnote{The code will be made public upon publication.}

%\section*{Acknowledgment}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\clearpage
\begin{appendices}

\section{Traffic participants detection}

%\begin{table*}[H]\centering
%\small{
%\ra{1.}
%%\begin{tabular}{@{}llllllllllll@{}}\toprule
%\begin{tabu}{@{}llllllllllll@{}}\toprule
%[-1pt] \tabucline[1pt]{1-12}
%\multicolumn{2}{c}{conf}  & 0.5  & 0.55 & 0.6 &  0.65 & 0.7  & 0.75 & 0.8 &  0.85 & 0.9  & 0.95\\
%\hline
%\multirow{3}{*}{Faster-RCNN}
%                    &   precision & 81.2  & 81.2  & 81.1 & 81.1 & 81.1 & 81.0 & 81.0 & 80.7 & 80.5 & 80.0 \\
%                    &   recall    & 91.4  & 91.4  & 91.3 & 91.3 & 91.3 & 91.3 & 91.3 & 90.9 & 90.7 & 90.2\\
%                    &   f-score   & 86.0  & 86.0  & 86.0 & 86.0 & 86.0 & 86.0 & 86.0 & 85.5 & 85.3 & 84.8\\
%                    \cline{2-12}
%\multirow{3}{*}{SSD (NMS:0.60)}&   precision &  82.1  & 81.9  & 81.8  & 81.8  & 81.6  & 81.1 & 80.9  & 80.4 & 80.0  & 79.2\\
%                            &    recall   & \textbf{92.7} & 92.5 & 92.3  & 92.3 & 92.0 & 91.5 & 91.3 & 90.7  & 90.2 & 89.4\\
%                            &    f-score  & 87.1  & 86.9  & 86.7   &86.7&86.5& 86.0  & 85.8 &85.2 & 84.8 &84.0\\
%                    \cline{2-12}
%\multirow{3 }{*}{SSD (NMS:0.45)}&   precision & \textbf{92.0 } & 91.8 & 91.7 & 91.4 & 91.1 & 91.0 & 90.5 & 90.2 & 89.4 & 87.4\\
%                                &    recall  & 92.3  & 92.1 & 92.0 & 91.8 & 91.4 & 91.3 & 90.8 & 90.5 & 89.7 & 87.7\\
%                                &    f-score  & \textbf{92.2 } & 92.0 & 91.8 & 91.6 & 91.3 & 91.1 & 91.1 & 90.4 & 89.6 & 87.6\\
%
%\bottomrule
%\end{tabu}
%%\end{tabular}
%\caption{
%Comparison of SSD and Faster-RCNN for vehicle detection on the collected SYNTHIA dataset.
%It can be seen that:
%(1) both meta-architectures are robust to the cut-off confident thresholds.
%(2) SSD is more competitive in the collected dataset when targets of interest are large.
%}
%\label{tab:ssd_fasterrcnn}
%}
%\end{table*}



\begin{table}[H]\centering
\small{
\ra{1.}
%\begin{tabular}{@{}llllllllllll@{}}\toprule
\begin{tabu}{@{}lllll@{}}\toprule
[-1pt] \tabucline[1pt]{1-5}
\multicolumn{2}{c}{conf}                       & 0.50    &0.70   & 0.95\\ \hline
\multirow{3}{*}{Faster-RCNN}
                                &   precision & 81.2    & 81.1  & 80.0\\
                                &   recall    & 91.4    & 91.3  & 90.2\\
                                &   f-score   & 86.0    & 86.0  & 84.8\\
                                    \cline{2-5}
\multirow{3}{*}{SSD (NMS:0.60)} &   precision &  82.1   & 81.6  & 79.2 \\
                                &   recall    & \textbf{92.7} & 92.0 & 89.4 \\
                                &   f-score   & 87.1 &86.5  &84.0\\
                                    \cline{2-5}
\multirow{3 }{*}{SSD (NMS:0.45)}&   precision & \textbf{92.0} & 91.1 & 87.4\\
                                &   recall  & 92.3 & 91.4 & 87.7\\
                                &   f-score  & \textbf{92.2}& 91.3 & 87.6\\
\bottomrule
\end{tabu}
%\end{tabular}
\caption{
Comparison of SSD and Faster-RCNN for vehicle detection on the collected SYNTHIA dataset.
It can be seen that:
(1) both meta-architectures are robust to the cut-off confident thresholds.
(2) SSD is more competitive in the collected dataset when targets of interest are comparatively plarge.
}
\label{tab:ssd_fasterrcnn}
}
\end{table}

\section{Instance association via tracking}
\begin{figure}[h]
        \centering
        \includegraphics[width=0.4\textwidth]{figures/double_detection.png}
        \caption{
        \small{Instance association can help to disambiguate the false positive by enforcing temporal consistency requirement. Due to the high recall requirement for vehicle detection, there could be correspondingly higher false positive rate (\emph{e.g.}, double detection of red boxed in the far left). Instance association across multiple frames will discard the spurious detection while remain temporal consistency.}
        }
        \label{fig:Instance_association}
\end{figure}

\section{2D and 3D occupancy grid}
\begin{figure}[t]
        \centering
        \includegraphics[width=0.5\textwidth]{figures/evaluation.pdf}
        \caption{
        \small{The evaluation of 2D and 3D prediction. 2D: The bounding box is set to cover the whole vehicle. 3D: The depth channel is combined with 2D bounding box to predict 3D occupancy.}
        }
        \label{fig:evaluation}
\end{figure}

\end{appendices}

\end{document}
